{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW9XeTa1GRVz"
   },
   "source": [
    "*Завдання 1: Розбийте MNIST на тренувальну та навчальну вибірки (60k + 10k). Натренуйте Random Forest classifier ти виміряйте час тренування. Обчисліть точність на тестовій вибірці.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FwUvN4HGsPF",
    "outputId": "23abc5a8-e985-4993-edba-9f001d03ec69",
    "ExecuteTime": {
     "end_time": "2025-10-23T16:16:01.449505Z",
     "start_time": "2025-10-23T16:16:01.444802Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:16:01.835Z",
     "start_time": "2025-10-23T16:16:01.461483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Завантаження даних\n",
    "with open('mnist_data.pkl', 'rb') as f:\n",
    "    mnist = pickle.load(f)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"].astype(np.uint8)\n",
    "\n",
    "# Розбиття на тренувальну (60k) та тестову (10k) вибірки\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]\n",
    "\n",
    "print(f\"Тренувальна вибірка: {X_train.shape}\")\n",
    "print(f\"Тестова вибірка: {X_test.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренувальна вибірка: (60000, 784)\n",
      "Тестова вибірка: (10000, 784)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "був файл локально з попереднього дз тому так і відкриваю і розбиваю\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:18:34.410283Z",
     "start_time": "2025-10-23T16:18:30.718184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Тренування Random Forest\n",
    "rf_no_pca = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_no_pca.fit(X_train, y_train)\n",
    "time_rf_no_pca = time.time() - start\n",
    "\n",
    "# Обчислення точності\n",
    "y_pred = rf_no_pca.predict(X_test)\n",
    "acc_rf_no_pca = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Розмірність даних: {X_train.shape[1]}\")\n",
    "print(f\"Час тренування: {time_rf_no_pca:.2f} сек\")\n",
    "print(f\"Точність: {acc_rf_no_pca:.4f} ({acc_rf_no_pca*100:.2f}%)\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розмірність даних: 784\n",
      "Час тренування: 3.66 сек\n",
      "Точність: 0.9691 (96.91%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "я довго думав чому в мене без пса моделька швидше тренується , думаю на n_jobs=-1 бо паралелізація при меншій розмірності більш погано працює або я просто якусь фігню знизу зробив"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzxht0sEGRVz"
   },
   "source": [
    "*Завдання 2: Застосуйте PCA з поясненою дисперсією 95%. Повторіть тренування. Порівняйте час тренування та точніть з попередньою вправою.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:16:06.031941Z",
     "start_time": "2025-10-23T16:16:05.676339Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Застосування PCA\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "time_pca = time.time() - start\n",
    "\n",
    "print(f\"Час PCA: {time_pca:.2f} сек\")\n",
    "print(f\"Оригінальна розмірність: {X_train.shape[1]}\")\n",
    "print(f\"Нова розмірність: {X_train_pca.shape[1]}\")\n",
    "print(f\"Кількість компонент: {pca.n_components_}\")\n",
    "print(f\"Пояснена дисперсія: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"Редукція: {(1 - X_train_pca.shape[1]/X_train.shape[1])*100:.1f}%\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Час PCA: 0.35 сек\n",
      "Оригінальна розмірність: 784\n",
      "Нова розмірність: 154\n",
      "Кількість компонент: 154\n",
      "Пояснена дисперсія: 0.9502\n",
      "Редукція: 80.4%\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Пояснена дисперсія: 0.9502\n",
    "це значить що ми зберегли 95% даних, при зменшенні розмірності на 80%. Дууууже круто для величезних датасетів"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:20:07.800026Z",
     "start_time": "2025-10-23T16:19:56.102532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Тренування Random Forest на PCA даних\n",
    "rf_with_pca = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_with_pca.fit(X_train_pca, y_train)\n",
    "time_rf_pca = time.time() - start\n",
    "\n",
    "# Обчислення точності\n",
    "y_pred_pca = rf_with_pca.predict(X_test_pca)\n",
    "acc_rf_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"Розмірність даних: {X_train_pca.shape[1]}\")\n",
    "print(f\"Час тренування: {time_rf_pca:.2f} сек\")\n",
    "print(f\"Точність: {acc_rf_pca:.4f} ({acc_rf_pca*100:.2f}%)\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розмірність даних: 154\n",
      "Час тренування: 11.66 сек\n",
      "Точність: 0.9487 (94.87%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ну от як таке може бути с пса я не розумію але ладно. Точність наче впала як і повинна була. Ну тепер в нас фічі втратились і є кластери. Розмірність точно зменшилася але ЧОМУ воно довше працює"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:22:17.131139Z",
     "start_time": "2025-10-23T16:22:17.128063Z"
    }
   },
   "cell_type": "code",
   "source": "X_test_pca.shape",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 154)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ну короче я не знаю, нормалізувати дані для рандом форесту наче не дуже треба, я пробував там гірші показники. У висновку я мав написати що пса пришвидшує але походу даних мало для пришвидшення)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "А чому в мене з пса довше код раниться((("
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icS_xMs5GRV0"
   },
   "source": [
    "*Завдання 3: Повторіть ці ж кроки з логістичною регресією. Зробіть висновки.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:16:24.456567Z",
     "start_time": "2025-10-23T16:16:18.054604Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_no_pca = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_no_pca.fit(X_train_scaled, y_train)\n",
    "lr_no_pca_time = time.time() - start_time\n",
    "\n",
    "y_pred_lr_no_pca = lr_no_pca.predict(X_test_scaled)\n",
    "lr_no_pca_accuracy = accuracy_score(y_test, y_pred_lr_no_pca)\n",
    "\n",
    "print(f\"Час тренування: {lr_no_pca_time:.2f} сек\")\n",
    "print(f\"Точність: {lr_no_pca_accuracy:.4f} ({lr_no_pca_accuracy*100:.2f}%)\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Час тренування: 6.22 сек\n",
      "Точність: 0.9216 (92.16%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "робимо все те саме що і зверху"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:16:25.989877Z",
     "start_time": "2025-10-23T16:16:24.477158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Нормалізація PCA даних\n",
    "scaler_pca = StandardScaler()\n",
    "X_train_pca_scaled = scaler_pca.fit_transform(X_train_pca)\n",
    "X_test_pca_scaled = scaler_pca.transform(X_test_pca)\n",
    "\n",
    "lr_with_pca = LogisticRegression(max_iter=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_with_pca.fit(X_train_pca_scaled, y_train)\n",
    "lr_pca_time = time.time() - start_time\n",
    "\n",
    "y_pred_lr_pca = lr_with_pca.predict(X_test_pca_scaled)\n",
    "lr_pca_accuracy = accuracy_score(y_test, y_pred_lr_pca)\n",
    "\n",
    "print(f\"Час тренування: {lr_pca_time:.2f} сек\")\n",
    "print(f\"Точність: {lr_pca_accuracy:.4f} ({lr_pca_accuracy*100:.2f}%)\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Час тренування: 1.47 сек\n",
      "Точність: 0.9237 (92.37%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:25:57.935891Z",
     "start_time": "2025-10-23T16:25:57.930778Z"
    }
   },
   "cell_type": "code",
   "source": "X_test_pca_scaled.shape",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 154)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ну тут все гуд працює і результати як і мають бути з часом тренування"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Висновки:\n",
    "\n",
    "1. Вплив PCA\n",
    "   - Зменшення розмірності: 784 до 154 компонент (80% редукція)\n",
    "   - Random Forest: прискорення НЕ відбулося((( але мало бути\n",
    "   - Logistic Regression: прискорення по часу в 3 рази десь\n",
    "   - Втрата точності мінімальна (1-2%) для обох випадків\n",
    "\n",
    "2. Порівняння моделей:\n",
    "   - Random Forest точніший (~96%)\n",
    "   - Logistic Regression швидший\n",
    "   - PCA критично важливий для роботи з величезними датасетами задля прискорення. Але не завжди підходить бо очевидний мінус - втрата фічів.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
